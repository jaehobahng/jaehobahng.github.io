---
layout: splash
title: "XGBoost"
categories: R
tag: coding
---
# XGBoost(Extreme Gradient Boosting)
<img src="\assets\images\XGBoost\Diagram.png" alt="Alt text">
Image Source : https://bcho.tistory.com/1354
<br/>
 - Both classification and regression are possible in the XGBoost model
 - A boosting method that builds on the error of the prior models
 - The dataset is put into the initial XGBoost model
 - When the misclassifications or the residuals of the linear regression are calculated, the errors are weighted on a stronger level and put into the model again for better results.
 - Repeat putting the errors into the model with different weights until errors are minimized

<pre>
Code Example : 
    Import Data : 
        library(MASS)
        data(Pima.tr)
        data(Pima.te)
        pima <- rbind(Pima.tr, Pima.te)
        set.seed(502)
        ind <- sample(2, nrow(pima), replace = TRUE, prob = c(0.7, 0.3))
        pima.train <- pima[ind == 1, ]
        pima.test <- pima[ind == 2, ]

    Parameter Setting for Model : 
        grid = expand.grid(
        nrounds = c(75, 100),
        colsample_bytree = 1,    #Default 1
        min_child_weight = 1,    #Degault 1
        eta = c(0.01, 0.1, 0.3), #Default 0.3(learning rate) / the higher it is the more vulnerable for overfitting
        gamma = c(0.5, 0.25),    #The reduction of risk minimization that decides the additional split of the leaf node(larger value = less overfitting)
        subsample = 0.5,         #default 1 : The sampling ratio for the weak learner
        max_depth = c(2, 3)      
        )
        grid

        cntrl = caret::trainControl(
        method = "cv",
        number = 5,
        verboseIter = TRUE,
        returnData = FALSE,
        returnResamp = "final"                                                        
        )

    Train model with given parameters :  
        set.seed(1)
        train.xgb = caret::train(
        x = pima.train[, 1:7],
        y = ,pima.train[, 8],
        trControl = cntrl,
        tuneGrid = grid,
        method = "xgbTree"
        )

        train.xgb
        param <- list(  objective           = "binary:logistic",         #reg:linear / multi:softmax / multi:softprob
                        booster             = "gbtree",
                        eval_metric         = "error",                   #Not necessary when training a regression model
                        eta                 = 0.1, 
                        max_depth           = 2, 
                        subsample           = 0.5,
                        colsample_bytree    = 1,
                        gamma               = 0.5,
                        min_child_weight = 2
        )

        x <- as.matrix(pima.train[, 1:7])
        y <- ifelse(pima.train$type == "Yes", 1, 0)
        train.mat <- xgboost::xgb.DMatrix(data = x, 
                                label = y)

        library(xgboost)
        set.seed(1)
        xgb.fit <- xgb.train(params = param, data = train.mat, nrounds = 75)
        xgb.fit
        pred <- predict(xgb.fit, x)

    Show importance of variables : 
        # summary(pred)
        # head(pred)
        # head(y)
        impMatrix <- xgb.importance(feature_names = dimnames(x)[[2]], 
                                            model = xgb.fit)
        impMatrix 
        xgb.plot.importance(impMatrix, main = "Gain by Feature")
        
    Evaluate Model : 
        library(InformationValue)
        pred <- predict(xgb.fit, x)
        optimalCutoff(y, pred)

        #Input Test data
        pima.testMat <- as.matrix(pima.test[, 1:7])
        xgb.pima.test <- predict(xgb.fit, pima.testMat)
        y.test <- ifelse(pima.test$type == "Yes", 1, 0)

        optimalCutoff(y.test, xgb.pima.test)
        confusionMatrix(y.test, xgb.pima.test, threshold = 0.39)
        1 - misClassError(y.test, xgb.pima.test, threshold = 0.39)
        plotROC(y.test, xgb.pima.test)

</pre>