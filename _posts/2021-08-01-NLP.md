---
layout: splash
title: "NLP(Text Analysis)"
categories: R
tag: coding
---
# Natural Language Processing(Text Analysis)
#### Change data format(Corpus, TermDocument Matrix, DataFrame)
 - Change data format according to usage

     - How to create and modify/inspect Corpus
<pre>
library(tm)

Creat Corpus : 
    text <- c("Crash dieting is not the best way to lose weight.", 
            "A vegeterian diet excludes all animal flesh(meat, poultry, seafood).",
            "Economists surveyed by Refinitiv expect the economy added 160,000 jobs.")
    text

    corpus.docs <- VCorpus(VectorSource(text)) 
    inspect(corpus.docs)

Read Corpus : 
    as.character(corpus.docs[[1]])
    lapply(corpus.docs,as.character)
    str(corpus.docs[[1]])

    #Read as one sentence
    paste(as.vector(unlist(lapply(corpus.docs, content))),collapse=' ')
</pre>

 - How to create and modify/inspect tidy(data frame)
<pre>
library(dplyr)
library(tidytext)

Create DataFrame : 
    text <- c("Crash dieting is not the best way to lose weight.", 
            "A vegeterian diet excludes all animal flesh(meat, poultry, seafood).",
            "Economists surveyed by Refinitiv expect the economy added 160,000 jobs.")
    source <- c('BBC','CNN','FOX')


    text.df <- tibble(source=source, text=text)

    # A word and source for each row of data
    unnest_tokens(tbl=text.df,output=word,input=text)

Delete Certain words : 
    word.removed <- tibble(word=c('http','bbc.in','1g0j4agg'))
    anti_join(tidy.docs, word.removed, by="word")

    tidy.docs <- tidy.docs %>%
    anti_join(word.removed, by="word")
    tidy.docs$word
</pre>

 - How to create and modify/inspect Term Document Matrix(Bag of words)
<pre>
library(tm)
library(tidytext)

    Create Text : 
        text <- c("Crash dieting is not the best way to lose weight.", 
                "A vegeterian diet excludes all animal flesh(meat, poultry, seafood).",
                "Economists surveyed by Refinitiv expect the economy added 160,000 jobs.")
        source <- c('BBC','CNN','FOX')

    Transform to Corpus and preprocess text : 
        corpus.docs <- VCorpus(VectorSource(text))
        
    Create TermDocumentMatrix
        corpus.dtm <- TermDocumentMatrix(corpus.docs,
                                        control=list(wordLengths=c(2,Inf)))
        corpus.dtm
        
    Inspect TDM : 
        nTerms(corpus.dtm) #number of words
        Terms(corpus.dtm)  #print words

        nDocs(corpus.dtm)  #number of documents
        Docs(corpus.dtm)   #name of documents

        rownames(corpus.dtm) <- c('BBC','CNN','FOX') #change rownames

        inspect(corpus.dtm)   #Word frequency for each document
        inspect(corpus.dtm[1:2,10:15])  #Indexing is possible

        #tidy 행렬로도 변환 가능
        library(tidytext)
        tidy(corpus.dtm)
</pre>



 - Preprocess Text
     - Punctuation, upper/lower case, stopwords etc.
<pre>
    address.corpus <- tm_map(address.corpus, content_transformer(tolower))

    sort(stopwords('english'))
    mystopwords <- c(stopwords('english'), 'can','must')
    address.corpus <- tm_map(address.corpus, removeWords, mystopwords)

    address.corpus <- tm_map(address.corpus, removePunctuation)
    address.corpus <- tm_map(address.corpus, removeNumbers)
    address.corpus <- tm_map(address.corpus, stripWhitespace)
    address.corpus <- tm_map(address.corpus, content_transformer(trimws))
    lapply(address.corpus[1],content)

    address.corpus <- tm_map(address.corpus, content_transformer(gsub),
                            pattern = "america|americas|american|americans",
                            replacement="america")
</pre>


### TF-IDF(Term Frequency-Inverse Document Frequency)
 - TF(Term Frequency)
     - How often a text appears in a documnet

 - IDF(Inverse Document Frequency)<br/>
    <img src="\assets\images\NLP\idf_equation.jpg" alt="Alt text">
     - n = total number of documents
     - dj = number of documents a word appears
    <br/>
     - idf = 0 : The word has no effect in describing the documents
     - When idf is larger, the word is unique in describing some documents

 - TF-IDF<br/>
    <img src="\assets\images\NLP\tfidf_equation.jpg" alt="Alt text">
     - Term Frequency X Inverse Document Frequency
     - How uniquely significant a word is for a document
     - The larger the tf-idf, the more unique a word is to a certain document

<pre>
Code example(cont.) : 
    Corpus → Document Term Matrix(with tf-idf)
        address.dtm2 <- TermDocumentMatrix(address.corpus,
                                        control=list(weighting=weightTfIdf))

        colnames(address.dtm2) <- c("Clinton","Bush","Obama","Trump","Biden")
        inspect(address.dtm2)


    Create Dataframe with tf-idf for each word in each document : 
        address.tfidf <- tidy(address.dtm2) %>%
            mutate(tf_idf=count, count=NULL)

        address.tfidf <- address.tfidf %>%
            mutate(document=factor(document,
                                    levels=c("Clinton","Bush","Obama","Trump","Biden"))) %>%
            arrange(desc(tf_idf))%>%
            group_by(document) %>%
            top_n(n=10, wt=tf_idf) %>%
            arrange(document,desc(tf_idf)) %>%
            ungroup()

        address.tfidf

    Visualize tf-idf : 
        ggplot(address.tfidf,aes(reorder_within(x=term, 
                                                by=tf_idf, 
                                                within=document),
                                y=tf_idf,fill=document)) + 
        geom_col(show.legend=FALSE) + 
        facet_wrap(~document,ncol=2,scales="free") + 
        labs(x=NULL, y="tf_idf") + 
        scale_x_reordered() +
        coord_flip()

    Calculate tf-idf : 
        address.words <- tidy(address.dtm)

        address.words <- address.words %>%
            tidytext::bind_tf_idf(term=term, document=document,n=count)

        address.words %>% arrange(desc(tf))
</pre>

### Analyze using visualization
<pre>
Code Example : 
library(quanteda)
library(tidytext)
library(tibble)
library(dplyr)
library(tm)

    Import Data : 
        data_corpus_inaugural
        summary(data_corpus_inaugural)
        class(data_corpus_inaugural)   #tm Corpus와는 다름

        #When a text is in the form of a list, convert it to a corpus before beginning
        corpus.docs <- VCorpus(VectorSource(text))

    Preprocess Corpus Data : Use tidy to group by Presidents and reconvert to courpus
        us.president.address <- tidy(data_corpus_inaugural) %>%
            filter(Year > 1990) %>%
            group_by(President,FirstName) %>%
            summarise_all(list(~trimws(paste(.,collapse=' ')))) %>%
            arrange(Year) %>%
            ungroup()
        <img src="\assets\images\NLP\us.president.address.png" alt="Alt text">

        us.president.address <- us.president.address %>%
            select(text, everything()) %>%
            add_column(doc_id=1:nrow(.), .before=1)  #id 칼럼 처음에 추가

        address.corpus <- VCorpus(DataframeSource(us.president.address))
        address.corpus

        lapply(address.corpus[1],content) #View saved texts
        
        
    Process Text : capitals, punctuation, spaces etc.
        address.corpus <- tm_map(address.corpus, content_transformer(tolower))

        sort(stopwords('english'))
        mystopwords <- c(stopwords('english'), 'can','must')
        address.corpus <- tm_map(address.corpus, removeWords, mystopwords)

        address.corpus <- tm_map(address.corpus, removePunctuation)
        address.corpus <- tm_map(address.corpus, removeNumbers)
        address.corpus <- tm_map(address.corpus, stripWhitespace)
        address.corpus <- tm_map(address.corpus, content_transformer(trimws))
        lapply(address.corpus[1],content)

        address.corpus <- tm_map(address.corpus, content_transformer(gsub),
                                pattern = "america|americas|american|americans",
                                replacement="america")
        
    Corpus → Documnet Term Matrix(DTM)
        address.dtm <- DocumentTermMatrix(address.corpus)
        inspect(address.dtm)

        findFreqTerms(address.dtm, lowfreq=40)
        findFreqTerms(address.dtm, lowfreq=40,highfreq=100)
        
    DTM → Datafrmae
        termfreq <- colSums(as.matrix(address.dtm))
        length(termfreq)
        termfreq[order(termfreq,decreasing=TRUE)]
            <img src="\assets\images\NLP\termfreq.png" alt="Alt text">

        termfreq.df <- data.frame(word=names(termfreq), frequency=termfreq)
        head(termfreq.df[order(termfreq.df[,2],decreasing=TRUE),],n=10)
            <img src="\assets\images\NLP\termfreq.df.png" alt="Alt text">

        library(ggplot2)

    Creat Plot with ggplot : 
        ggplot(subset(termfreq.df,frequency>40),
            aes(x=reorder(word,desc(frequency)),y=frequency,fill=word)) + 
        geom_col(color="dimgray", width=0.6, show.legend=FALSE) + 
        geom_text(aes(label=frequency),vjust=-0.5)

    Create Wordcloud : 
        set.seed(123)
        library(wordcloud)
        library(RColorBrewer)
        wordcloud(words=names(termfreq),freq=termfreq,
                scale=c(4,0.5),                 #Range of word size
                min.freq=10,                    #Minimum frequency
                rot.per=0.1,                    #10% = vertical
                colors=brewer.pal(6,"Dark2"),
                random.color=FALSE)             #Color based on frequency of word


                
</pre>

